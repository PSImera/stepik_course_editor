import yaml
import re
from pathlib import Path
from datetime import datetime
from typing import List, Tuple

import tiktoken
from langchain_openai import ChatOpenAI
from langchain_core.prompts import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)

from src.config import EditorConfig
from src.lesson_memory import LessonMemory

import warnings

warnings.filterwarnings("ignore")


class LessonsEditor:
    def __init__(self, config: EditorConfig = EditorConfig()):
        self.config = config
        self.llm = ChatOpenAI(
            base_url=self.config.base_url,
            model=self.config.model_name,
            temperature=self.config.temperature,
            timeout=self.config.llm_timeout,
        )

        with open(config.promts_yaml, "r", encoding="utf-8") as f:
            self.prompts = yaml.safe_load(f)

        if config.enable_lessons_memory:
            print("\nüìö –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑—ã —É—Ä–æ–∫–æ–≤ –∫—É—Ä—Å–∞...")
            self.lessons_db = LessonMemory()
            print("‚úÖ –ë–∞–∑–∞ —É—Ä–æ–∫–æ–≤ –≥–æ—Ç–æ–≤–∞")

    def _find_html_files(self, directory: str) -> List[Path]:
        """–ü–æ–∏—Å–∫ –≤—Å–µ—Ö HTML —Ñ–∞–π–ª–æ–≤ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"""
        html_files = []
        path = Path(directory)

        if not path.exists():
            raise ValueError(f"–ü–∞–ø–∫–∞ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {directory}")

        for file_path in path.rglob("*.html"):
            html_files.append(file_path)
        for file_path in path.rglob("*.htm"):
            html_files.append(file_path)

        return sorted(html_files)

    def _read_file(self, file_path: Path) -> str:
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                return f.read()
        except UnicodeDecodeError:
            for encoding in ["cp1251", "latin-1", "iso-8859-1"]:
                try:
                    with open(file_path, "r", encoding=encoding) as f:
                        return f.read()
                except UnicodeDecodeError:
                    continue
            raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª {file_path}")

    def _clean_markdown(self, text: str) -> str:
        """–£–¥–∞–ª—è–µ—Ç markdown —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLM"""
        text = text.strip()
        if text.startswith("```html"):
            text = text[7:]
        if text.startswith("```"):
            text = text[3:]
        if text.endswith("```"):
            text = text[:-3]
        return text.strip()

    def _make_prompt(self, content, coords):
        """–°–æ–±–∏—Ä–∞–µ—Ç –ø—Ä–æ–º–ø—Ç –∏–∑ —à–∞–±–ª–æ–Ω–∞"""

        system_current = self.prompts["rewrite_lesson"]["system"]["current"]
        task_current = self.prompts["rewrite_lesson"]["task"]["current"]
        system_prompt = self.prompts["rewrite_lesson"]["system"][system_current]
        task_prompt = self.prompts["rewrite_lesson"]["task"][task_current]

        from_memory_context = ""
        if self.config.enable_lessons_memory:
            print(f"   üìö –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —É—Ä–æ–∫–æ–≤. {coords}")
            from_memory_context = self.lessons_db.prev_lessons_context(
                coords, self.config.top_k_lessons
            )
            if from_memory_context:
                rag_lessons_context_tokens = self._estimate_tokens(from_memory_context)
                print(
                    f"   ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω –∫–æ–Ω—Ç–µ–∫—Å—Ç —É—Ä–æ–∫–æ–≤ ({rag_lessons_context_tokens} —Ç–æ–∫–µ–Ω–æ–≤)"
                )
            else:
                print("   ‚ÑπÔ∏è –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —É—Ä–æ–∫–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω")

        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
        chat_template = ChatPromptTemplate.from_messages(
            [
                SystemMessagePromptTemplate.from_template(system_prompt),
                HumanMessagePromptTemplate.from_template(task_prompt),
            ]
        )

        prompt = chat_template.invoke(
            {"lesson": content, "lessons_memory": from_memory_context}
        )

        return prompt

    def _clean_heading(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ –∑–∞–≥–æ—Ç–ª–æ–≤–∫–∞ –æ—Ç —ç–º–æ–¥–∂–∏ —Å–∏–º–≤–æ–ª–æ–≤"""
        text = re.sub(r"[üîπüî∏‚ú®üéØüìåüí°üöÄ‚ö°üî•]+\s*", "", text)
        return text.strip()

    def _parse_file_structure(
        self, file_path: Path
    ) -> Tuple[int, int, int, str, str, str]:
        """
        –ü–∞—Ä—Å–∏—Ç int –Ω–æ–º–µ—Ä–∞ –º–æ–¥—É–ª—è, —É—Ä–æ–∫–∞, —à–∞–≥–∞ –∏ –∏—Ö –Ω–∞–∑–≤–∞–Ω–∏—è –∏–∑ –ø—É—Ç–∏ —Ñ–∞–π–ª–∞
        """
        filename = file_path.stem

        match = re.match(r"(\d+)-(\d+)-(\d+)_(.+)", filename)
        if not match:
            raise ValueError(f"–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞: {filename}")

        module_num = int(match.group(1))
        lesson_num = int(match.group(2))
        step_num = int(match.group(3))
        step_title = match.group(4).strip()
        step_title = self._clean_heading(step_title)

        lesson_folder = file_path.parent
        lesson_match = re.match(r"(\d+)\.\s*(.+)", lesson_folder.name)
        if lesson_match:
            folder_lesson_num = int(lesson_match.group(1))
            lesson_title = lesson_match.group(2).strip()

            if folder_lesson_num != lesson_num:
                print(
                    f"‚ö†Ô∏è –ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –Ω–æ–º–µ—Ä–∞ —É—Ä–æ–∫–∞: —Ñ–∞–π–ª={lesson_num}, –ø–∞–ø–∫–∞={folder_lesson_num} ({lesson_folder.name})"
                )
        else:
            lesson_title = lesson_folder.name
            print(f"‚ö†Ô∏è –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –ø–∞–ø–∫–∏ —É—Ä–æ–∫–∞: {lesson_folder.name}")

        module_folder = lesson_folder.parent
        module_match = re.match(r"(\d+)\.\s*(.+)", module_folder.name)
        if module_match:
            folder_module_num = int(module_match.group(1))
            module_title = module_match.group(2).strip()

            if folder_module_num != module_num:
                print(
                    f"‚ö†Ô∏è –ù–µ—Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –Ω–æ–º–µ—Ä–∞ –º–æ–¥—É–ª—è: —Ñ–∞–π–ª={module_num}, –ø–∞–ø–∫–∞={folder_module_num} ({module_folder.name})"
                )
        else:
            module_title = module_folder.name
            print(f"‚ö†Ô∏è –ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –ø–∞–ø–∫–∏ –º–æ–¥—É–ª—è: {module_folder.name}")

        return module_num, lesson_num, step_num, module_title, lesson_title, step_title

    def _estimate_tokens(self, text):
        """
        –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ
        """
        if self.config.count_tokens_for:
            try:
                enc = tiktoken.encoding_for_model(self.config.count_tokens_for)
            except:
                enc = tiktoken.get_encoding("cl100k_base")
        else:
            enc = tiktoken.get_encoding("cl100k_base")
        return len(enc.encode(text))

    def _calculate_cost(self, input_tokens: int, output_tokens: int = 0) -> int:
        """
        –°—á–∏—Ç–∞–µ—Ç —Ü–µ–Ωe –∑–∞–ø—Ä–æ—Å–∞
        :param input_tokens: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –≤–æ–ø—Ä–æ—Å–µ
        :param output_tokens: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤ –≤ –æ—Ç–≤–µ—Ç–µ
        """
        prices = self.config.prices[self.config.count_tokens_for]
        cost_input = (input_tokens / 1_000_000) * prices["input"]
        cost_output = (output_tokens / 1_000_000) * prices["output"]
        return cost_input + cost_output

    def _process_single_file(
        self, file_path: Path, structure: Tuple[int, int, int, str, str, str]
    ):
        """
        –†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞

        :param file_path: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É
        :param structure: –Ω–æ–º–µ—Ä–∞ –º–æ–¥—É–ª—è, —É—Ä–æ–∫–∞, —à–∞–≥–∞ –∏ –∏—Ö –Ω–∞–∑–≤–∞–Ω–∏—è
        """
        try:
            print(f"\nüìÑ {file_path.name}")
            module_num, lesson_num, step_num, _, _, _ = structure

            text_content = self.lessons_db._parse_lesson_html(file_path)
            chunks = self.lessons_db._split_lesson_text(
                text_content, file_path, structure
            )
            print(
                f"   üìñ –ó–∞–≥—Ä—É–∂–µ–Ω —É—Ä–æ–∫ (–ü–æ–∑–∏—Ü–∏—è: {module_num}-{lesson_num}-{step_num})"
            )

            edited_chanks = []
            edited_chanks_count = 0
            total_cost = 0
            print(f"   üìñ –£—Ä–æ–∫ —Ä–∞–∑–±–∏—Ç –Ω–∞ {len(chunks)} —á–∞—Å—Ç–µ–π")
            for i, chunk in enumerate(chunks):
                print(f"\n      –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç—å [{i+1}/{len(chunks)}]\n")
                md = chunk.metadata
                coords = (
                    md["module_num"],
                    md["lesson_num"],
                    md["step_num"],
                    md["section_num"],
                    md["subsection_num"],
                )
                prompt = self._make_prompt(chunk.page_content, coords)

                estimated_tokens = self._estimate_tokens(
                    "".join([m.content for m in prompt.to_messages()])
                )
                context_info = (
                    " (—Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º)" if self.config.enable_lessons_memory else ""
                )
                print(
                    f"      üìè –†–∞–∑–º–µ—Ä –ø—Ä–æ–º–ø—Ç–∞{context_info}: {estimated_tokens} —Ç–æ–∫–µ–Ω–æ–≤"
                )

                max_input_tokens = self.config.context_length - 2500
                if estimated_tokens > max_input_tokens:
                    raise Exception(
                        f"      ‚ö†Ô∏è –ö–æ–Ω—Ç–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π: ({estimated_tokens} —Ç–æ–∫–µ–Ω–æ–≤, –ª–∏–º–∏—Ç - {max_input_tokens})"
                    )

                print(f"      üîÑ –û—Ç–ø—Ä–∞–≤–ª—è—é –∑–∞–ø—Ä–æ—Å LLM...")
                response = self.llm.invoke(prompt)
                total_cost += self._calculate_cost(
                    response.usage_metadata["input_tokens"],
                    response.usage_metadata["output_tokens"],
                )

                edited_chunk = self._clean_markdown(response.content)
                edited_chanks.append(edited_chunk)

                # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —á–∞–Ω–∫ –≤ –ë–î —É—Ä–æ–∫–æ–≤
                if self.config.enable_lessons_memory and self.lessons_db:
                    print(f"      üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–∞–Ω–∫–∞ –≤ RAG...")
                    chunk.page_content = edited_chunk
                    chunk.metadata.update({"timestamp": datetime.now().isoformat()})
                    self.lessons_db._add_chunk_to_db(chunk)

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è
                if chunk.page_content != edited_chunk:
                    edited_chanks_count += 1

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–π–ª
            edited_lesson = "\n".join(edited_chanks)
            with open(file_path, "w", encoding="utf-8") as f:
                f.write(edited_lesson)

            print(f"‚úÖ –ò–∑–º–µ–Ω—ë–Ω")
            print(f"üíµ –¶–µ–Ω–∞ –∑–∞–ø—Ä–æ—Å–∞\–æ—Ç–≤–µ—Ç–∞. {total_cost}$")
        except Exception as e:
            print(f"‚ùå {str(e)}")

    def process_file(self, module_num: int, lesson_num: int, step_num: int):
        """
        –†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –æ–¥–∏–Ω —à–∞–≥. –†–∞–∑–±–∏–≤–∞–µ—Ç —à–∞–≥ –Ω–∞ —á–∞–Ω–∫–∏ –ø–æ h1/h2 –∑–∞–≥–æ–ª–æ–≤–∫–∞–º,
        –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–æ–æ—á–µ—Ä—ë–¥–Ω–æ –¥–∞–≤–∞—è –º–æ–¥–µ–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ k —á–∞–Ω–∫–æ–≤ –≤ —Ç.—á. —Å –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —à–∞–≥–æ–≤

        :param module_num: –Ω–æ–º–µ—Ä –º–æ–¥—É–ª—è
        :param lesson_num: –Ω–æ–º–µ—Ä —É—Ä–æ–∫–∞
        :param step_num: –Ω–æ–º–µ—Ä —à–∞–≥–∞
        """
        html_files = self._find_html_files(self.config.course_save_folder)
        for file_path in html_files:
            structure = self._parse_file_structure(file_path)
            if (
                structure[0] == module_num
                and structure[1] == lesson_num
                and structure[2] == step_num
            ):
                found_file = file_path
                break
        self._process_single_file(found_file, structure)

    def edit_text(self, module_num: int, lesson_num: int, step_num: int):
        """
        –ü—Ä–æ—Å—Ç–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —É—Ä–æ–∫–∞, –±–µ–∑ –≤—Å—è–∫–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ —Ä–∞–∑–±–∏–µ–Ω–∏—è
        –ë–æ–ª—å—à–µ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –∑–∞–¥–∞–Ω–∏–π –≤—Ä–æ–¥–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –æ—à–∏–±–æ–∫, –Ω–µ–∂–µ–ª–∏ —á–µ–º –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–Ω–∏—è —Å —Å–ª–æ–∂–Ω—ã–º–∑–∞–¥–∞–Ω–∏–µ–º

        :param module_num: –Ω–æ–º–µ—Ä –º–æ–¥—É–ª—è
        :param lesson_num: –Ω–æ–º–µ—Ä —É—Ä–æ–∫–∞
        :param step_num: –Ω–æ–º–µ—Ä —à–∞–≥–∞
        """
        html_files = self._find_html_files(self.config.course_save_folder)
        for file_path in html_files:
            structure = self._parse_file_structure(file_path)
            if (
                structure[0] == module_num
                and structure[1] == lesson_num
                and structure[2] == step_num
            ):
                found_file = file_path
                break
        try:
            module_num, lesson_num, step_num, _, _, _ = structure

            with open(found_file, "r", encoding="utf-8") as f:
                content = f.read()

            system_current = self.prompts["edit_text"]["system"]["current"]
            task_current = self.prompts["edit_text"]["task"]["current"]
            system_prompt = self.prompts["edit_text"]["system"][system_current]
            task_prompt = self.prompts["edit_text"]["task"][task_current]

            chat_template = ChatPromptTemplate.from_messages(
                [
                    SystemMessagePromptTemplate.from_template(system_prompt),
                    HumanMessagePromptTemplate.from_template(task_prompt),
                ]
            )

            prompt = chat_template.invoke({"lesson": content})

            estimated_tokens = self._estimate_tokens(
                "".join([m.content for m in prompt.to_messages()])
            )
            print(f"      üìè –†–∞–∑–º–µ—Ä –ø—Ä–æ–º–ø—Ç–∞: {estimated_tokens} —Ç–æ–∫–µ–Ω–æ–≤")

            max_input_tokens = self.config.context_length - 2500
            if estimated_tokens > max_input_tokens:
                raise Exception(
                    f"      ‚ö†Ô∏è –ö–æ–Ω—Ç–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π: ({estimated_tokens} —Ç–æ–∫–µ–Ω–æ–≤, –ª–∏–º–∏—Ç - {max_input_tokens})"
                )

            print(f"      üîÑ –û—Ç–ø—Ä–∞–≤–ª—è—é –∑–∞–ø—Ä–æ—Å LLM...")
            response = self.llm.invoke(prompt)
            total_cost = self._calculate_cost(
                response.usage_metadata["input_tokens"],
                response.usage_metadata["output_tokens"],
            )

            with open(found_file, "w", encoding="utf-8") as f:
                f.write(response.content)

            print(f"‚úÖ –ò–∑–º–µ–Ω—ë–Ω")
            print(
                f"üíµ –¶–µ–Ω–∞ –∑–∞–ø—Ä–æ—Å–∞\–æ—Ç–≤–µ—Ç–∞. {total_cost}$, {self._rub(total_cost)}RUB, {self._amd(total_cost)}AMD"
            )
        except Exception as e:
            print(f"‚ùå {str(e)}")

    def ask_about_text(self, module_num: int, lesson_num: int, step_num: int):
        """
        –î–∞—ë—Ç—Å—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ —Ç–µ–∫—Å—Ç —à–∞–≥–∞ –∏ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –ø—Ä–æ–º–ø—Ç–æ–º –º–æ–¥–µ–ª–∏ –æ—Ç–≤–µ—á–∞–µ—Ç.
        –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –∑–∞–¥–∞–Ω–∏–π –≤—Ä–æ–¥–µ "—Å–æ—Å—Ç–∞–≤—å —Ç–µ—Å—Ç—ã"

        :param module_num: –Ω–æ–º–µ—Ä –º–æ–¥—É–ª—è
        :param lesson_num: –Ω–æ–º–µ—Ä —É—Ä–æ–∫–∞
        :param step_num: –Ω–æ–º–µ—Ä —à–∞–≥–∞
        """
        html_files = self._find_html_files(self.config.course_save_folder)
        for file_path in html_files:
            structure = self._parse_file_structure(file_path)
            if (
                structure[0] == module_num
                and structure[1] == lesson_num
                and structure[2] == step_num
            ):
                found_file = file_path
                break
        try:
            module_num, lesson_num, step_num, _, _, _ = structure

            with open(found_file, "r", encoding="utf-8") as f:
                content = f.read()

            system_current = self.prompts["ask_about_text"]["system"]["current"]
            task_current = self.prompts["ask_about_text"]["task"]["current"]
            system_prompt = self.prompts["ask_about_text"]["system"][system_current]
            task_prompt = self.prompts["ask_about_text"]["task"][task_current]

            chat_template = ChatPromptTemplate.from_messages(
                [
                    SystemMessagePromptTemplate.from_template(system_prompt),
                    HumanMessagePromptTemplate.from_template(task_prompt),
                ]
            )

            prompt = chat_template.invoke({"lesson": content})

            estimated_tokens = self._estimate_tokens(
                "".join([m.content for m in prompt.to_messages()])
            )
            print(f"      üìè –†–∞–∑–º–µ—Ä –ø—Ä–æ–º–ø—Ç–∞: {estimated_tokens} —Ç–æ–∫–µ–Ω–æ–≤")

            max_input_tokens = self.config.context_length - 2500
            if estimated_tokens > max_input_tokens:
                raise Exception(
                    f"      ‚ö†Ô∏è –ö–æ–Ω—Ç–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π: ({estimated_tokens} —Ç–æ–∫–µ–Ω–æ–≤, –ª–∏–º–∏—Ç - {max_input_tokens})"
                )

            print(f"      üîÑ –û—Ç–ø—Ä–∞–≤–ª—è—é –∑–∞–ø—Ä–æ—Å LLM...")
            response = self.llm.invoke(prompt)
            total_cost = self._calculate_cost(
                response.usage_metadata["input_tokens"],
                response.usage_metadata["output_tokens"],
            )
            print(f"üíµ –¶–µ–Ω–∞ –∑–∞–ø—Ä–æ—Å–∞\–æ—Ç–≤–µ—Ç–∞. {total_cost}$")
            print("\n\n", response.content, "\n\n")

        except Exception as e:
            print(f"‚ùå {str(e)}")


if __name__ == "__main__":
    agent = LessonsEditor()
    agent.process_file(module_num=1, lesson_num=1, step_num=1)
